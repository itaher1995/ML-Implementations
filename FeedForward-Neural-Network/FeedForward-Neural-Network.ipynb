{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_,_,_,X_tst, X_trn, X_val, Y_tst, Y_trn, Y_val = loadmat('dataset.mat').values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 200)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Based on the above output, the number of input neurons for the feedforward neural network should be 2, one for each input. The number of output layers is 1 if you use a sigmoid activation function for the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork():\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def __ACTIVATION__(self,z_i,activation='ReLU'):\n",
    "        '''\n",
    "        Function for implementing activation function. For the purposes of this program the following\n",
    "        activation functions will be used:\n",
    "        \n",
    "        sigmoid: y = 1/(1 + e^-z)\n",
    "        tanh: y= (e^z - e^-z)/(e^z + e^-z)\n",
    "        ReLU: y=max(0,z_i)\n",
    "        \n",
    "        This code is important for the forward propogation aspect of this algorithm.\n",
    "        '''\n",
    "        if activation == \"sigmoid\":\n",
    "            a_i = 1/(1+np.exp(z_i))\n",
    "        elif activation == \"softmax\":\n",
    "            a_i = np.exp(z_i)/float(np.sum(np.exp(z_i)))\n",
    "        elif activation == \"tanh\":\n",
    "            a_i = (np.exp(z_i)-np.exp(-z_i))/(np.exp(z_i)+np.exp(-z_i))\n",
    "        else:\n",
    "            a_i = np.maximum(z_i,0)\n",
    "        return a_i\n",
    "\n",
    "    def __FORWARDPROPOGATION__(self,W,b,X,n=3):\n",
    "        '''\n",
    "        Initialize a = X\n",
    "        for (i in 1:n-2)\n",
    "        z_i <- W_i-1*a_i-1 + b_i-1\n",
    "        a_i <- f(z_i)\n",
    "        \n",
    "        return a_i\n",
    "        '''\n",
    "\n",
    "        actArray=[]\n",
    "        a = X #for input layer a[0] = z[0] = X\n",
    "        actArray.append(a)\n",
    "        for i in range(1,n-1): #shallow network so this should only iterate once. \n",
    "            z_i = np.dot(W[i],a) + b[i] #input into the activation function\n",
    "            a = self.__ACTIVATION__(z_i) #default is ReLU(z_i)\n",
    "            actArray.append(a)\n",
    "        return actArray\n",
    "    \n",
    "    def __DERIVATIVE__(self,output,activation):\n",
    "        '''\n",
    "        Calculates the derivative for the following functions:\n",
    "        \n",
    "        sigmoid\n",
    "        tanh\n",
    "        ReLU\n",
    "        '''\n",
    "        if activation == 'sigmoid':\n",
    "            derivative = output * (1.0-output)\n",
    "        elif activation == 'ReLU':\n",
    "            derivative = (output>0)*1\n",
    "        else:\n",
    "            derivative=1-output**2\n",
    "        return derivative\n",
    "    def __BACKPROPOGATION__(self,X,Y,output,weights,biases,alpha,m,lambda_,n):\n",
    "        '''\n",
    "        Implementation of backpropogation algorithm.\n",
    "        '''\n",
    "    \n",
    "        \n",
    "        # delta to change the weights \n",
    "        delta_W = {i:np.zeros(weights[i].shape) for i in range(1,n)}\n",
    "        delta_b = {i:np.zeros(biases[i].shape) for i in range(1,n)}    \n",
    "        \n",
    "        error = output[-1]-Y\n",
    "        delta3=error*self.__DERIVATIVE__(output[-1],\"sigmoid\")#element-wise d3 = e - f'(z_i)\n",
    "        \n",
    "        #Hidden Layer\n",
    "        dJdwHidden = np.dot(delta3,output[1].T)#dJ/dW2 = <d3,dz2>\n",
    "        delta_W[2] = delta_W[2] + dJdwHidden\n",
    "        delta_b[2] = np.sum(delta3)\n",
    "        weights[2] = weights[2] - alpha*((1/m)*delta_W[2] + lambda_*weights[2])\n",
    "        biases[2] = biases[2] - alpha*((1/m)*delta_b[2])\n",
    "        \n",
    "        #Input Layer\n",
    "        delta2=np.multiply(error,self.__DERIVATIVE__(output[1],\"RELU\"))\n",
    "        dJdwHidden = np.dot(delta2,output[0].T)\n",
    "        delta_W[1] = delta_W[1] + dJdwHidden\n",
    "        delta_b[1] = np.sum(delta2)\n",
    "        weights[1] = weights[1] - alpha*((1/m)*delta_W[1] + lambda_*weights[1])\n",
    "        biases[1] = biases[1] - alpha*((1/m)*delta_b[1])\n",
    "        \n",
    "        return weights,biases\n",
    "\n",
    "        \n",
    "    def train(self,X,Y,S_n,alpha,m,lambda_,num_iters,n=3):\n",
    "        '''\n",
    "        Training the feedforward neural network using the backpropogation algorithm.\n",
    "        '''      \n",
    "        X_copy=scale(X,axis=1)\n",
    "        \n",
    "        #random initialization of weights and biases from a normal distribution. To be updated during backprop\n",
    "        W={i:np.random.normal(0,0.01,(S_n[i],S_n[i-1])) for i in range(1,n)}\n",
    "        b={i:np.random.normal(0,0.01,(S_n[i],1)) for i in range(1,n)}\n",
    "\n",
    "        for i in range(num_iters): #use a finite set of iterations for now\n",
    "            output = self.__FORWARDPROPOGATION__(W,b,X_copy) #compiles but might be wrong\n",
    "            output.append(1/(1+np.exp(np.dot(W[n-1],output[-1])+b[n-1]))) #sigmoid activation function for output layer\n",
    "            W,b = self.__BACKPROPOGATION__(_,Y,output,W,b,alpha,m,lambda_,n)\n",
    "\n",
    "        return W,b\n",
    "    \n",
    "    def test(self,X,W,b,n=3):\n",
    "        X_copy=scale(X,axis=1)\n",
    "        output = self.__FORWARDPROPOGATION__(W,b,X_copy) #compiles but might be wrong\n",
    "        output.append(1/(1+np.exp(np.dot(W[n-1],output[-1])+b[n-1]))) #sigmoid activation function for output layer\n",
    "        return output[-1]\n",
    "        \n",
    "        \n",
    "    \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationError(y_pred,y_true):\n",
    "    '''\n",
    "    number wrong/total number of samples'''\n",
    "    wrong = 0\n",
    "    total = len(y_true)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]!=y_true[i]:\n",
    "            wrong+=1\n",
    "    return wrong/total\n",
    "\n",
    "def bestLambda(lambdaList,numHidden):\n",
    "    bestE=1\n",
    "    for l in lambdaList:\n",
    "        for i in range(1000):\n",
    "            model = FeedForwardNeuralNetwork()\n",
    "            W,b = model.train(X=X_trn,Y=Y_trn,S_n=[2,numHidden,1],alpha=0.0001,m=1,lambda_=l,num_iters=100,n=3)\n",
    "            output = model.test(X_val,W,b)\n",
    "            output = np.where(output>0.5,1,0)\n",
    "            e=classificationError(output[0],Y_val[0])\n",
    "            if e<bestE:\n",
    "                bestLambda=l\n",
    "                bestE=e\n",
    "                bestW=W\n",
    "                bestb=b\n",
    "    print(f'The best lambda was {bestLambda} with a classification error of {bestE}')\n",
    "    return bestLambda, bestW, bestb\n",
    "\n",
    "def bestNN(lambdaList, hiddenList):\n",
    "    for n in [2,10]:\n",
    "        modelLambda, modelWeight, modelBias = bestLambda([1,1.5,2,2.5,3],n)\n",
    "        print(f\"weights for {n}:{modelWeight} \\n bias for {n}:{modelBias}\")\n",
    "        model = FeedForwardNeuralNetwork()\n",
    "        outputTrain = model.test(X_trn,W,b)\n",
    "        outputTrain = np.where(output>0.5,1,0)\n",
    "        eTrain = classificationError(output[0],Y_trn[0])\n",
    "\n",
    "        outputTest = model.test(X_tst,W,b)\n",
    "        outputTest = np.where(output>0.5,1,0)\n",
    "        eTest = classificationError(output[0],Y_tst[0])\n",
    "        print(f\"for n={n} the classification on the training data was {eTrain} and on the test set was {eTest}\")\n",
    "\n",
    "def plotClasses(X,Y):\n",
    "    x00 = [X[0][i] for i in range(len(X[0])) if Y[0][i]==0]\n",
    "    x01 = [X[0][i] for i in range(len(X[0])) if Y[0][i]==1]\n",
    "    x10 = [X[1][i] for i in range(len(X[0])) if Y[0][i]==0]\n",
    "    x11 = [X[1][i] for i in range(len(X[0])) if Y[0][i]==1]\n",
    "    \n",
    "    plt.scatter(x00,x10, marker='s',c='r')\n",
    "    plt.scatter(x01,x11,marker='o',c='b')\n",
    "    \n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best lambda was 1 with a classification error of 0.25\n",
      "weights for 2:{1: array([[ 0.0137064 , -0.01655113],\n",
      "       [ 0.00585685, -0.00948039]]), 2: array([[-0.0234075 , -0.00500669]])} \n",
      " bias for 2:{1: array([[0.0023692 ],\n",
      "       [0.01709551]]), 2: array([[0.00058917]])}\n",
      "for n=2 the classification on the training data was 0.0 and on the test set was 0.16666666666666666\n",
      "The best lambda was 2 with a classification error of 0.0875\n",
      "weights for 10:{1: array([[ 0.01158871, -0.02220623],\n",
      "       [-0.00857389, -0.02079491],\n",
      "       [ 0.01985767, -0.0213294 ],\n",
      "       [ 0.01373495, -0.0302635 ],\n",
      "       [ 0.01186941, -0.02218892],\n",
      "       [ 0.01692276, -0.007281  ],\n",
      "       [ 0.02238533, -0.02596694],\n",
      "       [ 0.01572654, -0.00179527],\n",
      "       [ 0.00408027, -0.01642721],\n",
      "       [ 0.00043755, -0.00939175]]), 2: array([[-0.00729142,  0.01099659,  0.01109704,  0.0036223 , -0.01381038,\n",
      "         0.02285406, -0.00083967,  0.00148704, -0.01248456, -0.00510608]])} \n",
      " bias for 10:{1: array([[ 0.00809756],\n",
      "       [-0.00800585],\n",
      "       [ 0.0111352 ],\n",
      "       [-0.0054627 ],\n",
      "       [ 0.00133424],\n",
      "       [-0.0177636 ],\n",
      "       [-0.00907961],\n",
      "       [-0.00546324],\n",
      "       [-0.01086175],\n",
      "       [-0.00154447]]), 2: array([[-1.50319166e-05]])}\n",
      "for n=10 the classification on the training data was 0.0 and on the test set was 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "bestNN([1,1.5,2,2.5,3],[2,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEKCAYAAADuEgmxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24f40204b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotClasses(X_trn,Y_trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: No the data is not linearly separable and both the models with $S_{n}$ = 2 and $S_{n}$=10 having low classification errors (0 for the training data.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
